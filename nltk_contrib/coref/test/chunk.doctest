    >>> from nltk.tree import Tree
    >>> from nltk.corpus import conll2000
    >>> from nltk_contrib.coref.chunk import tokens2tree
    >>>
    >>> iob_toks = conll2000.iob_sents()[15]
    >>> tokens2tree(iob_toks) #doctest: +NORMALIZE_WHITESPACE
    Tree('S', [('Meanwhile', 'RB'), (',', ','), Tree('NP', [('overall', 'JJ'), ('evidence', 'NN')]), Tree('PP', [('on', 'IN')]), Tree('NP', [('the', 'DT'), ('economy', 'NN')]), Tree('VP', [('remains', 'VBZ')]), ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')])

    >>> from nltk_contrib.coref.chunk import NaiveBayesChunkTagger
    >>>
    >>> iob_train_toks = conll2000.iob_sents()[0:25]
    >>> iob_test_toks = conll2000.iob_sents()[25:30]
    >>>
    >>> chunker = NaiveBayesChunkTagger.train(iob_train_toks)
    >>> chunkscore = chunker.test(iob_test_toks) #doctest: +NORMALIZE_WHITESPACE
    Precision: 0.71
    Recall:    0.64
    Accuracy:  0.76
    F-measure: 0.67

    >>> tagged_toks = conll2000.tagged_sents()[15:16]
    >>> chunker.chunk(tagged_toks[0]) #doctest: +NORMALIZE_WHITESPACE    
    [('Meanwhile', 'RB'), (',', ','), [('overall', 'JJ'), ('evidence', 'NN')], [('on', 'IN')], [('the', 'DT'), ('economy', 'NN')], [('remains', 'VBZ')], ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')]

    >>> chunker.batch_chunk(tagged_toks[:1]) #doctest: +NORMALIZE_WHITESPACE    
    [[('Meanwhile', 'RB'), (',', ','), [('overall', 'JJ'), ('evidence', 'NN')], [('on', 'IN')], [('the', 'DT'), ('economy', 'NN')], [('remains', 'VBZ')], ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')]]
    
    >>> from nltk_contrib.coref.chunk import CRFChunkTagger
    >>>
    >>> chunker = None
    >>> chunker = CRFChunkTagger.train(iob_train_toks, trace=0)
    [Found java: /usr/bin/java]
    >>> chunkscore = chunker.test(iob_test_toks) #doctest: +NORMALIZE_WHITESPACE
    Precision: 0.76
    Recall:    0.74
    Accuracy:  0.85
    F-measure: 0.75
    
    >>> chunker.chunk(tagged_toks[0]) #doctest: +NORMALIZE_WHITESPACE
    [('Meanwhile', 'RB'), (',', ','), [('overall', 'JJ'), ('evidence', 'NN')], [('on', 'IN')], [('the', 'DT'), ('economy', 'NN')], [('remains', 'VBZ')], ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')]        
    
    >>> chunker.batch_chunk(tagged_toks[:1]) #doctest: +NORMALIZE_WHITESPACE
    [[('Meanwhile', 'RB'), (',', ','), [('overall', 'JJ'), ('evidence', 'NN')], [('on', 'IN')], [('the', 'DT'), ('economy', 'NN')], [('remains', 'VBZ')], ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')]]
    
    >>> from nltk_contrib.coref.chunk import MaxentChunkTagger
    >>>    
    >>> chunker = None
    >>> chunker = MaxentChunkTagger.train(iob_train_toks)
    [Found megam: /usr/local/bin/megam.opt]
    >>> chunkscore = chunker.test(iob_test_toks) #doctest: +NORMALIZE_WHITESPACE
    Precision: 0.74
    Recall:    0.64
    Accuracy:  0.81
    F-measure: 0.69    
    
    >>> chunker.chunk(tagged_toks[0]) #doctest: +NORMALIZE_WHITESPACE
    [('Meanwhile', 'RB'), (',', ','), [('overall', 'JJ'), ('evidence', 'NN')], [('on', 'IN')], [('the', 'DT'), ('economy', 'NN')], [('remains', 'VBZ')], ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')]    

    >>> chunker.batch_chunk(tagged_toks[:1]) #doctest: +NORMALIZE_WHITESPACE
    [[('Meanwhile', 'RB'), (',', ','), [('overall', 'JJ'), ('evidence', 'NN')], [('on', 'IN')], [('the', 'DT'), ('economy', 'NN')], [('remains', 'VBZ')], ('fairly', 'RB'), ('clouded', 'VBN'), ('.', '.')]]